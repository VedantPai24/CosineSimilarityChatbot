{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca81529",
      "metadata": {
        "id": "bca81529"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"test.csv\", encoding='unicode_escape')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81ddbe6f",
      "metadata": {
        "id": "81ddbe6f"
      },
      "outputs": [],
      "source": [
        "questions_list = df['Questions'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64601d5e",
      "metadata": {
        "id": "64601d5e"
      },
      "outputs": [],
      "source": [
        "answers_list = df['Answers'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3bffd3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bffd3a",
        "outputId": "2836429f-1ca8-41ea-cba0-0f93ac545368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e900a08",
      "metadata": {
        "id": "5e900a08"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "194886bb",
      "metadata": {
        "id": "194886bb"
      },
      "outputs": [],
      "source": [
        "def preprocess_with_stopwords(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d658f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d658f6",
        "outputId": "6292ea33-aa38-462a-9a4b-f4e19ef42135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0320d12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "a0320d12",
        "outputId": "13ba0331-5492-4b86-ea40-74bcb93ed502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_text: who is m dhoni\n",
            "similarities: [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "max_similarity: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I can't answer this question.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
        "\n",
        "def get_response(text):\n",
        "    processed_text = preprocess_with_stopwords(text)\n",
        "    print(\"processed_text:\", processed_text)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    print(\"similarities:\", similarities)\n",
        "    max_similarity = np.max(similarities)\n",
        "    print(\"max_similarity:\", max_similarity)\n",
        "    if max_similarity > 0.6:\n",
        "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
        "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
        "\n",
        "        target_answers = []\n",
        "        for q in high_similarity_questions:\n",
        "            q_index = questions_list.index(q)\n",
        "            target_answers.append(answers_list[q_index])\n",
        "        print(target_answers)\n",
        "\n",
        "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
        "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "        closest = np.argmax(final_similarities)\n",
        "        return target_answers[closest]\n",
        "    else:\n",
        "        return \"I can't answer this question.\"\n",
        "\n",
        "get_response('Who is ms dhoni?')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
        "\n",
        "def get_response(text):\n",
        "    processed_text = preprocess_with_stopwords(text)\n",
        "    print(\"processed_text:\", processed_text)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    print(\"similarities:\", similarities)\n",
        "    max_similarity = np.max(similarities)\n",
        "    print(\"max_similarity:\", max_similarity)\n",
        "    if max_similarity > 0.6:\n",
        "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
        "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
        "\n",
        "        target_answers = []\n",
        "        for q in high_similarity_questions:\n",
        "            q_index = questions_list.index(q)\n",
        "            target_answers.append(answers_list[q_index])\n",
        "        print(target_answers)\n",
        "\n",
        "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
        "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "        closest = np.argmax(final_similarities)\n",
        "        return target_answers[closest]\n",
        "    else:\n",
        "        return \"I can't answer this question.\"\n",
        "\n",
        "get_response('what is machine learning')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NUoJPaJUX9co",
        "outputId": "665663eb-e438-446f-c343-909d76721f89"
      },
      "id": "NUoJPaJUX9co",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_text: what is machin learn\n",
            "similarities: [[0.         0.         0.         0.         0.77627227 0.\n",
            "  0.         0.        ]]\n",
            "max_similarity: 0.7762722680124386\n",
            "high_similarity_questions: ['What is the role of machine learning in data analytics?']\n",
            "['Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.']\n",
            "processed_text_with_stopwords: what is machin learn\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gingerit.gingerit import GingerIt\n",
        "\n",
        "text = 'What is Data Anlytics'\n",
        "\n",
        "parser = GingerIt()\n",
        "corrected_text = parser.parse(text)\n",
        "\n",
        "print(corrected_text['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHBV_uDhrSB",
        "outputId": "0ce83955-94ea-49e0-e7fc-4dfd215bb010"
      },
      "id": "YHHBV_uDhrSB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Data Anlytics?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uJ_s0VrsjGcs"
      },
      "id": "uJ_s0VrsjGcs",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}